{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10 # input dimension (word embedding) D\n",
    "hidden_size = 30 # hidden dimension H\n",
    "batch_size = 3\n",
    "length = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size, hidden_size,num_layers=1,bias=False,nonlinearity='tanh', batch_first=True, dropout=0, bidirectional=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 30])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Variable(torch.ones(batch_size,length,input_size)) # B,T,D  <= batch_first\n",
    "hidden = Variable(torch.zeros(1,batch_size,hidden_size)) # 1,B,H    (num_layers * num_directions, batch, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden_2 = rnn(input_,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 30])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output, hidden = rnn(input_) \n",
    "\n",
    "# (배치 사이즈, 시퀀스 길이, input 차원)을 가지는 Input \n",
    "# (1,배치 사이즈, hidden 차원)을 가지는 초기 hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 30])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 30])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 30])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 30])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2400,  0.4409, -0.6815,  0.3815,  0.4817,  0.3543,  0.0278,\n",
       "          -0.2422, -0.5285,  0.2401, -0.5547, -0.2543, -0.5245, -0.4616,\n",
       "          -0.1528,  0.2695,  0.4874, -0.3386,  0.4287, -0.0242, -0.0146,\n",
       "          -0.2488,  0.4442,  0.3694,  0.2878,  0.3819,  0.3784,  0.2036,\n",
       "           0.3754,  0.4077],\n",
       "         [-0.1207,  0.1482, -0.6327,  0.3872,  0.5229,  0.3736, -0.3267,\n",
       "          -0.2428, -0.2329,  0.2017, -0.6798,  0.2052, -0.5195, -0.5068,\n",
       "           0.1413,  0.5225,  0.5724, -0.3839,  0.3542, -0.4187,  0.0185,\n",
       "          -0.0204,  0.5256,  0.3115,  0.4012, -0.0352,  0.4341,  0.3334,\n",
       "           0.3463,  0.5739],\n",
       "         [-0.1083,  0.2041, -0.6793,  0.5248,  0.6526,  0.4556, -0.3271,\n",
       "          -0.1001, -0.2185,  0.1026, -0.7892,  0.2265, -0.3555, -0.5174,\n",
       "           0.0561,  0.5115,  0.4459, -0.4866,  0.4238, -0.2722,  0.0156,\n",
       "          -0.1135,  0.4092,  0.2613,  0.4166,  0.0428,  0.3991,  0.3373,\n",
       "           0.4260,  0.5506],\n",
       "         [-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697]],\n",
       "\n",
       "        [[-0.2400,  0.4409, -0.6815,  0.3815,  0.4817,  0.3543,  0.0278,\n",
       "          -0.2422, -0.5285,  0.2401, -0.5547, -0.2543, -0.5245, -0.4616,\n",
       "          -0.1528,  0.2695,  0.4874, -0.3386,  0.4287, -0.0242, -0.0146,\n",
       "          -0.2488,  0.4442,  0.3694,  0.2878,  0.3819,  0.3784,  0.2036,\n",
       "           0.3754,  0.4077],\n",
       "         [-0.1207,  0.1482, -0.6327,  0.3872,  0.5229,  0.3736, -0.3267,\n",
       "          -0.2428, -0.2329,  0.2017, -0.6798,  0.2052, -0.5195, -0.5068,\n",
       "           0.1413,  0.5225,  0.5724, -0.3839,  0.3542, -0.4187,  0.0185,\n",
       "          -0.0204,  0.5256,  0.3115,  0.4012, -0.0352,  0.4341,  0.3334,\n",
       "           0.3463,  0.5739],\n",
       "         [-0.1083,  0.2041, -0.6793,  0.5248,  0.6526,  0.4556, -0.3271,\n",
       "          -0.1001, -0.2185,  0.1026, -0.7892,  0.2265, -0.3555, -0.5174,\n",
       "           0.0561,  0.5115,  0.4459, -0.4866,  0.4238, -0.2722,  0.0156,\n",
       "          -0.1135,  0.4092,  0.2613,  0.4166,  0.0428,  0.3991,  0.3373,\n",
       "           0.4260,  0.5506],\n",
       "         [-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697]],\n",
       "\n",
       "        [[-0.2400,  0.4409, -0.6815,  0.3815,  0.4817,  0.3543,  0.0278,\n",
       "          -0.2422, -0.5285,  0.2401, -0.5547, -0.2543, -0.5245, -0.4616,\n",
       "          -0.1528,  0.2695,  0.4874, -0.3386,  0.4287, -0.0242, -0.0146,\n",
       "          -0.2488,  0.4442,  0.3694,  0.2878,  0.3819,  0.3784,  0.2036,\n",
       "           0.3754,  0.4077],\n",
       "         [-0.1207,  0.1482, -0.6327,  0.3872,  0.5229,  0.3736, -0.3267,\n",
       "          -0.2428, -0.2329,  0.2017, -0.6798,  0.2052, -0.5195, -0.5068,\n",
       "           0.1413,  0.5225,  0.5724, -0.3839,  0.3542, -0.4187,  0.0185,\n",
       "          -0.0204,  0.5256,  0.3115,  0.4012, -0.0352,  0.4341,  0.3334,\n",
       "           0.3463,  0.5739],\n",
       "         [-0.1083,  0.2041, -0.6793,  0.5248,  0.6526,  0.4556, -0.3271,\n",
       "          -0.1001, -0.2185,  0.1026, -0.7892,  0.2265, -0.3555, -0.5174,\n",
       "           0.0561,  0.5115,  0.4459, -0.4866,  0.4238, -0.2722,  0.0156,\n",
       "          -0.1135,  0.4092,  0.2613,  0.4166,  0.0428,  0.3991,  0.3373,\n",
       "           0.4260,  0.5506],\n",
       "         [-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697],\n",
       "         [-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697],\n",
       "         [-0.0596,  0.2062, -0.6812,  0.5150,  0.6734,  0.4206, -0.3502,\n",
       "          -0.1237, -0.2323,  0.0551, -0.7724,  0.1610, -0.3251, -0.4898,\n",
       "           0.1142,  0.4912,  0.3822, -0.4616,  0.4632, -0.2536,  0.0102,\n",
       "          -0.1147,  0.4144,  0.3260,  0.5088,  0.0060,  0.3943,  0.3139,\n",
       "           0.3733,  0.5697]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 2\n",
    "T_horizon     = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,64)\n",
    "        self.lstm  = nn.LSTM(64,32)\n",
    "        self.fc_pi = nn.Linear(32,2)\n",
    "        self.fc_v  = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=2)\n",
    "        return prob, lstm_hidden\n",
    "    \n",
    "    def v(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, h_in_lst, h_out_lst, done_lst = [], [], [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, h_in, h_out, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            h_in_lst.append(h_in)\n",
    "            h_out_lst.append(h_out)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask,prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                         torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                         torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s,a,r,s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0]\n",
    "        \n",
    "    def train_net(self):\n",
    "        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()\n",
    "        first_hidden  = (h1_in.detach(), h2_in.detach())\n",
    "        second_hidden = (h1_out.detach(), h2_out.detach())\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            v_prime = self.v(s_prime, second_hidden).squeeze(1)\n",
    "            td_target = r + gamma * v_prime * done_mask\n",
    "            v_s = self.v(s, first_hidden).squeeze(1)\n",
    "            delta = td_target - v_s\n",
    "            delta = delta.detach().numpy()\n",
    "            \n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for item in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + item[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi, _ = self.pi(s, first_hidden)\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == log(exp(a)-exp(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v_s, td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward(retain_graph=True)\n",
    "            self.optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "model = PPO()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "\n",
    "for n_epi in range(10000):\n",
    "    h_out = (torch.zeros([1, 1, 32], dtype=torch.float), torch.zeros([1, 1, 32], dtype=torch.float))\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            h_in = h_out\n",
    "            prob, h_out = model.pi(torch.from_numpy(s).float(), h_in)\n",
    "            prob = prob.view(-1)\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, info = env.step(a)\n",
    "\n",
    "            model.put_data((s, a, r/100.0, s_prime, prob[a].item(), h_in, h_out, done))\n",
    "            s = s_prime\n",
    "\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "                    \n",
    "        model.train_net()\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,64)\n",
    "        self.lstm  = nn.LSTM(64,32)\n",
    "        self.fc_pi = nn.Linear(32,4)\n",
    "        self.fc_v  = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=2)\n",
    "        return prob, lstm_hidden\n",
    "    \n",
    "    def v(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, h_in_lst, h_out_lst, done_lst = [], [], [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, h_in, h_out, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            h_in_lst.append(h_in)\n",
    "            h_out_lst.append(h_out)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask,prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                         torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                         torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.data = []\n",
    "        return s,a,r,s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0]\n",
    "        \n",
    "    def train_net(self):\n",
    "        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()\n",
    "        first_hidden  = (h1_in.detach(), h2_in.detach())\n",
    "        second_hidden = (h1_out.detach(), h2_out.detach())\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            v_prime = self.v(s_prime, second_hidden).squeeze(1)\n",
    "            td_target = r + gamma * v_prime * done_mask\n",
    "            v_s = self.v(s, first_hidden).squeeze(1)\n",
    "            delta = td_target - v_s\n",
    "            delta = delta.detach().numpy()\n",
    "            \n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for item in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + item[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi, _ = self.pi(s, first_hidden)\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == log(exp(a)-exp(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v_s, td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward(retain_graph=True)\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PPO()\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
